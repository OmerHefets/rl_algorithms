{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATION = \"simulation\"\n",
    "ANALYTIC = \"analytic\"\n",
    "ITERATIVE = \"iterative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess():\n",
    "    def __init__(self, alg=SIMULATION, gamma=0.99, n_episodes=100, epsilon=0.01):\n",
    "        self.alg = alg\n",
    "        self.gamma = gamma\n",
    "        self.n_episodes = n_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.value_function = None\n",
    "        self._time_steps = 500\n",
    "    \n",
    "    \n",
    "    def fit(self, S, P, R):\n",
    "        \"\"\"\n",
    "        S: Integer that represent the amount of states\n",
    "        P: Transition probability matrix\n",
    "        R: List of rewards at the length of len(S) (reward for each state)\n",
    "        \"\"\"\n",
    "        # Set the value table\n",
    "        self.value_function = np.zeros(S)\n",
    "        \n",
    "        if self.alg == SIMULATION: \n",
    "            self._fit_simulation(S, P, R)\n",
    "        elif self.alg == ANALYTIC:\n",
    "            self._fit_analytical_solution(S, P, R)\n",
    "        else:\n",
    "            self._fit_iterative_solution(S, P, R)\n",
    "\n",
    "\n",
    "    \n",
    "    def value_func(self):\n",
    "        return self.value_function\n",
    "    \n",
    "    \n",
    "    def _fit_simulation(self, S, P, R):\n",
    "        for state in range(S):\n",
    "            state_reward = 0\n",
    "            for i in range(self.n_episodes):\n",
    "                # generate episode and episode reward\n",
    "                episode_reward = self._generate_episode(S, P, R, state)\n",
    "                # add the reward to state_reward\n",
    "                state_reward += episode_reward\n",
    "            \n",
    "            # normalize by n_episodes\n",
    "            state_reward /= self.n_episodes\n",
    "            # set v(s) = this value\n",
    "            self.value_function[state] = state_reward\n",
    "     \n",
    "    \n",
    "    def _fit_analytical_solution(self, S, P, R):\n",
    "        # Identity matrix the size of n_states\n",
    "        identity_matrix = np.identity(S)\n",
    "        \n",
    "        # Transform R to a vector\n",
    "        R_vector = np.asarray(R).reshape(-1, 1)\n",
    "        \n",
    "        # Perform the analytical solution\n",
    "        value_funct_vector = np.dot(np.linalg.inv(identity_matrix - self.gamma * P), R_vector)\n",
    "        \n",
    "        # Return the value function\n",
    "        self.value_function = value_funct_vector.tolist()\n",
    "    \n",
    "    \n",
    "    def _fit_iterative_solution(self, S, P, R):\n",
    "        # Init the value function\n",
    "        self.value_function = np.ones(S) * self.epsilon * 2\n",
    "        temp_value_function = np.zeros(S)\n",
    "        \n",
    "        while np.linalg.norm(self.value_function - temp_value_function) > self.epsilon:\n",
    "            self.value_function = temp_value_function.copy()\n",
    "            for state in range(S):\n",
    "                temp_value_function[state] = R[state] + self.gamma * np.dot(P[state], np.transpose(temp_value_function))\n",
    "\n",
    "    \n",
    "    def _generate_episode(self, S, P, R, state):\n",
    "        t = 0\n",
    "        episode_reward = 0\n",
    "        while t < self._time_steps:\n",
    "            # add reward from current state (with gamma!)\n",
    "            episode_reward += np.power(self.gamma, t) * R[state]\n",
    "            # move to a new state\n",
    "            state = np.random.choice(S, p=P[state])\n",
    "            # t++\n",
    "            t += 1\n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 2\n",
    "P = np.array([[0, 1], [1, 0]])\n",
    "R = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp = MarkovRewardProcess(gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.fit(S, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.73684211, 15.26315789])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp.value_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp2 = MarkovRewardProcess(alg=ANALYTIC, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp2.fit(S, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14.736842105263161], [15.263157894736846]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp2.value_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp3 = MarkovRewardProcess(alg=ITERATIVE, gamma=0.9, epsilon=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp3.fit(S, P ,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.73313698, 15.25982328])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp3.value_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08967741, 0.20728536, 0.21037018, 0.37600818],\n",
       "       [0.33755854, 0.99550407, 0.30595872, 0.83815765],\n",
       "       [0.8458791 , 0.87183648, 0.2208869 , 0.59762388],\n",
       "       [0.83535189, 0.55467284, 0.32918539, 0.62810763],\n",
       "       [0.95835326, 0.24146916, 0.26422433, 0.56791863]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33755854, 0.99550407, 0.30595872, 0.83815765])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=np.array([[1],[2],[3],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.59907344])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a[1],v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy and Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_ITER = \"policy_iteration\"\n",
    "VALUE_ITER = \"value_iteration\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Rover Env:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MRP Case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 7\n",
    "P = np.array([[0.6, 0.4, 0, 0, 0, 0, 0],\n",
    "             [0.4, 0.2, 0.4, 0, 0, 0, 0],\n",
    "             [0, 0.4, 0.2, 0.4, 0, 0, 0],\n",
    "             [0, 0, 0.4, 0.2, 0.4, 0, 0],\n",
    "             [0, 0, 0, 0.4, 0.2, 0.4, 0],\n",
    "             [0, 0, 0, 0, 0.4, 0.2, 0.4],\n",
    "             [0, 0, 0, 0, 0, 0.4, 0.6]])\n",
    "R = [1, 0, 0, 0, 0, 0, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.4, 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0.4, 0.2, 0.4, 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.4, 0.2, 0.4, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.4, 0.2, 0.4, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.4, 0.2, 0.4, 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.4, 0.2, 0.4],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0.4, 0.6]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp_rover = MarkovRewardProcess(gamma=0.5, alg=ANALYTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp_rover.fit(S, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.5342666565342837],\n",
       " [0.3699332978699934],\n",
       " [0.1304331838806863],\n",
       " [0.21701602959309496],\n",
       " [0.846138949288241],\n",
       " [3.59060924220399],\n",
       " [15.311602640629713]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp_rover.value_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MDP Case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 7\n",
    "A = 2 # left + right\n",
    "P_non_deterministic = np.array(\n",
    "            [[[0.9, 0.1, 0, 0, 0, 0, 0],\n",
    "             [0.9, 0.1, 0, 0, 0, 0, 0],\n",
    "             [0, 0, 0.9, 0.1, 0, 0, 0],\n",
    "             [0, 0, 0, 0.9, 0.1, 0, 0],\n",
    "             [0, 0, 0, 0, 0.9, 0.1, 0],\n",
    "             [0, 0, 0, 0, 0, 0.9, 0.1],\n",
    "             [0, 0, 0, 0, 0, 0.1, 0.9]],\n",
    "             [[0.1, 0.9, 0, 0, 0, 0, 0],\n",
    "             [0, 0.1, 0.9, 0, 0, 0, 0],\n",
    "             [0, 0, 0.1, 0.9, 0, 0, 0],\n",
    "             [0, 0, 0, 0.1, 0.9, 0, 0],\n",
    "             [0, 0, 0, 0, 0.1, 0.9, 0],\n",
    "             [0, 0, 0, 0, 0, 0.1, 0.9],\n",
    "             [0, 0, 0, 0, 0, 0.1, 0.9]]])\n",
    "R = [1, 0, 0, 0, 0, 0, 10]\n",
    "GAMMA = 0.99\n",
    "PI = [0, 1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, alg=POLICY_ITER, gamma=0.99, n_episodes=100, epsilon=0.01):\n",
    "        self.alg = alg\n",
    "        self.gamma = gamma\n",
    "        self.n_episodes = n_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.value_function = None\n",
    "        self._time_steps = 500\n",
    "        \n",
    "\n",
    "    def fit(self, S, P, R):\n",
    "        \"\"\"\n",
    "        S: Integer that represent the amount of states\n",
    "        R: List of rewards at the length of len(S) (reward for each state)\n",
    "        \"\"\"\n",
    "        # Set the value table\n",
    "        self.value_function = np.zeros(S)\n",
    "        \n",
    "        if self.alg == POLICY_ITER: \n",
    "            self._fit_XXX()\n",
    "        else:\n",
    "            self._fit_YYY()\n",
    "\n",
    "            \n",
    "    def policy_evaluation(self, S, P, R, A, policy, alg):\n",
    "        actions = [a for a in range(A)]\n",
    "        transition_matrix = self._build_transition_matrix_from_policy(S, P, policy)\n",
    "        mrp = MarkovRewardProcess(gamma=self.gamma, n_episodes=self.n_episodes, epsilon=self.epsilon)\n",
    "        mrp.fit(S, transition_matrix, R)\n",
    "        self.value_function = mrp.value_func()\n",
    "        \n",
    "    \n",
    "    def _build_transition_matrix_from_policy(self, S, P, policy):\n",
    "        transition_matrix = np.zeros((S, S))\n",
    "        for state, action in zip(range(S), policy):\n",
    "            transition_matrix[state] = P[action][state]\n",
    "            \n",
    "        return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MarkovDecisionProcess(gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.policy_evaluation(S, P_non_deterministic, R, A, policy=PI, alg=ANALYTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([432.52408488, 845.36253093, 858.24898904, 864.41090604,\n",
       "       874.45239674, 882.87265965, 897.86733738])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.value_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
