{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Reward Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMULATION = \"simulation\"\n",
    "ANALYTIC = \"analytic\"\n",
    "ITERATIVE = \"iterative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovRewardProcess():\n",
    "    def __init__(self, alg=SIMULATION, gamma=0.99, n_episodes=100, epsilon=0.01):\n",
    "        self.alg = alg\n",
    "        self.gamma = gamma\n",
    "        self.n_episodes = n_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.value_function = None\n",
    "        self._time_steps = 500\n",
    "    \n",
    "    \n",
    "    def fit(self, S, P, R):\n",
    "        \"\"\"\n",
    "        S: Integer that represent the amount of states\n",
    "        P: Transition probability matrix\n",
    "        R: List of rewards at the length of len(S) (reward for each state)\n",
    "        \"\"\"\n",
    "        # Set the value table\n",
    "        self.value_function = np.zeros(S)\n",
    "        \n",
    "        if self.alg == SIMULATION: \n",
    "            self._fit_simulation(S, P, R)\n",
    "        elif self.alg == ANALYTIC:\n",
    "            self._fit_analytical_solution(S, P, R)\n",
    "        else:\n",
    "            self._fit_iterative_solution(S, P, R)\n",
    "\n",
    "\n",
    "    \n",
    "    def value_func(self):\n",
    "        return np.asarray(self.value_function).reshape(1, -1)\n",
    "    \n",
    "    \n",
    "    def _fit_simulation(self, S, P, R):\n",
    "        for state in range(S):\n",
    "            state_reward = 0\n",
    "            for i in range(self.n_episodes):\n",
    "                # generate episode and episode reward\n",
    "                episode_reward = self._generate_episode(S, P, R, state)\n",
    "                # add the reward to state_reward\n",
    "                state_reward += episode_reward\n",
    "            \n",
    "            # normalize by n_episodes\n",
    "            state_reward /= self.n_episodes\n",
    "            # set v(s) = this value\n",
    "            self.value_function[state] = state_reward\n",
    "     \n",
    "    \n",
    "    def _fit_analytical_solution(self, S, P, R):\n",
    "        # Identity matrix the size of n_states\n",
    "        identity_matrix = np.identity(S)\n",
    "        \n",
    "        # Transform R to a vector\n",
    "        R_vector = np.asarray(R).reshape(-1, 1)\n",
    "        \n",
    "        # Perform the analytical solution\n",
    "        value_funct_vector = np.dot(np.linalg.inv(identity_matrix - self.gamma * P), R_vector)\n",
    "        \n",
    "        # Return the value function\n",
    "        self.value_function = value_funct_vector.tolist()\n",
    "    \n",
    "    \n",
    "    def _fit_iterative_solution(self, S, P, R):\n",
    "        # Init the value function\n",
    "        self.value_function = np.ones(S) * self.epsilon * 2\n",
    "        temp_value_function = np.zeros(S)\n",
    "        \n",
    "        while np.linalg.norm(self.value_function - temp_value_function) > self.epsilon:\n",
    "            self.value_function = temp_value_function.copy()\n",
    "            for state in range(S):\n",
    "                temp_value_function[state] = R[state] + self.gamma * np.dot(P[state], np.transpose(temp_value_function))\n",
    "\n",
    "    \n",
    "    def _generate_episode(self, S, P, R, state):\n",
    "        t = 0\n",
    "        episode_reward = 0\n",
    "        while t < self._time_steps:\n",
    "            # add reward from current state (with gamma!)\n",
    "            episode_reward += np.power(self.gamma, t) * R[state]\n",
    "            # move to a new state\n",
    "            state = np.random.choice(S, p=P[state])\n",
    "            # t++\n",
    "            t += 1\n",
    "        return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 2\n",
    "P = np.array([[0, 1], [1, 0]])\n",
    "R = [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp = MarkovRewardProcess(gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.fit(S, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.73684211, 15.26315789]])"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp.value_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp2 = MarkovRewardProcess(alg=ANALYTIC, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp2.fit(S, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[14.736842105263161], [15.263157894736846]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp2.value_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp3 = MarkovRewardProcess(alg=ITERATIVE, gamma=0.9, epsilon=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp3.fit(S, P ,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.73313698, 15.25982328])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp3.value_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08967741, 0.20728536, 0.21037018, 0.37600818],\n",
       "       [0.33755854, 0.99550407, 0.30595872, 0.83815765],\n",
       "       [0.8458791 , 0.87183648, 0.2208869 , 0.59762388],\n",
       "       [0.83535189, 0.55467284, 0.32918539, 0.62810763],\n",
       "       [0.95835326, 0.24146916, 0.26422433, 0.56791863]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33755854, 0.99550407, 0.30595872, 0.83815765])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "v=np.array([[1],[2],[3],[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.59907344])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a[1],v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy and Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLICY_ITER = \"policy_iteration\"\n",
    "VALUE_ITER = \"value_iteration\"\n",
    "POLICY_SEARCH = \"policy_search\" # Not implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Rover Env:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MRP Case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 7\n",
    "P = np.array([[0.6, 0.4, 0, 0, 0, 0, 0],\n",
    "             [0.4, 0.2, 0.4, 0, 0, 0, 0],\n",
    "             [0, 0.4, 0.2, 0.4, 0, 0, 0],\n",
    "             [0, 0, 0.4, 0.2, 0.4, 0, 0],\n",
    "             [0, 0, 0, 0.4, 0.2, 0.4, 0],\n",
    "             [0, 0, 0, 0, 0.4, 0.2, 0.4],\n",
    "             [0, 0, 0, 0, 0, 0.4, 0.6]])\n",
    "R = [1, 0, 0, 0, 0, 0, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6, 0.4, 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0.4, 0.2, 0.4, 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.4, 0.2, 0.4, 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.4, 0.2, 0.4, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0.4, 0.2, 0.4, 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.4, 0.2, 0.4],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0.4, 0.6]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp_rover = MarkovRewardProcess(gamma=0.5, alg=ANALYTIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp_rover.fit(S, P, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.5342666565342837],\n",
       " [0.3699332978699934],\n",
       " [0.1304331838806863],\n",
       " [0.21701602959309496],\n",
       " [0.846138949288241],\n",
       " [3.59060924220399],\n",
       " [15.311602640629713]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp_rover.value_func()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MDP Case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 7\n",
    "A = 2 # left + right\n",
    "P_non_deterministic = np.array(\n",
    "            [[[0.9, 0.1, 0, 0, 0, 0, 0],\n",
    "             [0.9, 0.1, 0, 0, 0, 0, 0],\n",
    "             [0, 0.9, 0.1, 0, 0, 0, 0],\n",
    "             [0, 0, 0.9, 0.1, 0, 0, 0],\n",
    "             [0, 0, 0, 0.9, 0.1, 0, 0],\n",
    "             [0, 0, 0, 0, 0.9, 0.1, 0],\n",
    "             [0, 0, 0, 0, 0, 0.9, 0.1]],\n",
    "             [[0.1, 0.9, 0, 0, 0, 0, 0],\n",
    "             [0, 0.1, 0.9, 0, 0, 0, 0],\n",
    "             [0, 0, 0.1, 0.9, 0, 0, 0],\n",
    "             [0, 0, 0, 0.1, 0.9, 0, 0],\n",
    "             [0, 0, 0, 0, 0.1, 0.9, 0],\n",
    "             [0, 0, 0, 0, 0, 0.1, 0.9],\n",
    "             [0, 0, 0, 0, 0, 0.1, 0.9]]])\n",
    "R = [1, 0, 0, 0, 0, 0, 10]\n",
    "GAMMA = 0.99\n",
    "PI = [1, 1, 1, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovDecisionProcess():\n",
    "    def __init__(self, alg=POLICY_ITER, eval_alg=ANALYTIC, gamma=0.99, n_episodes=100, epsilon=0.01):\n",
    "        self.alg = alg\n",
    "        self.evaluation_alg = eval_alg\n",
    "        self.gamma = gamma\n",
    "        self.n_episodes = n_episodes\n",
    "        self.epsilon = epsilon\n",
    "        self.value_function = None\n",
    "        self._time_steps = 500\n",
    "        self.best_policy = None\n",
    "        \n",
    "\n",
    "    def fit(self, S, P, R, A):\n",
    "        \"\"\"\n",
    "        S: Integer that represent the amount of states\n",
    "        R: List of rewards at the length of len(S) (reward for each state)\n",
    "        \"\"\"\n",
    "        # Set the value table\n",
    "        self.value_function = np.zeros(S)\n",
    "        \n",
    "        if self.alg == POLICY_ITER: \n",
    "            self._fit_policy_iteration(S, P, R, A)\n",
    "        elif self.alg == VALUE_ITER:\n",
    "            self._fit_policy_iteration(S, P, R, A)\n",
    "\n",
    "            \n",
    "    def _fit_policy_iteration(self, S, P, R, A):\n",
    "        # Get a random policy\n",
    "        policy = np.random.randint(A, size=S)\n",
    "        \n",
    "        # While policy changes:\n",
    "        while True:\n",
    "            # Get policy value\n",
    "            policy_value = self.policy_evaluation(S, P, R, A, policy)\n",
    "            \n",
    "            # Improve the policy\n",
    "            new_policy = self.policy_improvement(S, P, R, A, policy_value)\n",
    "            \n",
    "            # if the policy didn't change, then stop\n",
    "            if np.array_equal(policy, new_policy):\n",
    "                break\n",
    "            else:\n",
    "                policy = new_policy\n",
    "        \n",
    "        # Return this policy and it's value\n",
    "        self.value_function = policy_value\n",
    "        self.best_policy = policy\n",
    "            \n",
    "\n",
    "    def _fit_value_iteration(self, S, P, R, A):\n",
    "        policy = []\n",
    "        self.value_function = np.ones(S) * self.epsilon * 2\n",
    "        temp_value_function = np.zeros(S)\n",
    "        \n",
    "        while np.linalg.norm(self.value_function - temp_value_function) > self.epsilon:\n",
    "            self.value_function = temp_value_function.copy()\n",
    "            for state in range(S):\n",
    "                values = []\n",
    "                for action in range(A):\n",
    "                    action_value = R[state] + self.gamma * np.dot(P[action][state], np.transpose(self.value_function))\n",
    "                    values.append(action_value)\n",
    "                temp_value_function[state] = np.max(values)\n",
    "                \n",
    "        # Get the policy from the best values\n",
    "        for state in range(S):\n",
    "            values = []\n",
    "            for action in range(A):\n",
    "                action_value = R[state] + self.gamma * np.dot(P[action][state], np.transpose(value_function))\n",
    "                values.append(action_value)\n",
    "            policy.append(np.argmax(values))\n",
    "        \n",
    "        # Save the policy (We already saved the value of this policy)\n",
    "        self.best_policy = policy\n",
    "\n",
    "    def policy_improvement(self, S, P, R, A, value_function):\n",
    "        policy = []\n",
    "        for state in range(S):\n",
    "            values = []\n",
    "            for action in range(A):\n",
    "                action_value = R[state] + self.gamma * np.dot(P[action][state], np.transpose(value_function))\n",
    "                values.append(action_value)\n",
    "            policy.append(np.argmax(values))\n",
    "        return np.asarray(policy)\n",
    "    \n",
    "            \n",
    "    def policy_evaluation(self, S, P, R, A, policy):\n",
    "        transition_matrix = self._build_transition_matrix_from_policy(S, P, policy)\n",
    "        mrp = MarkovRewardProcess(alg=self.evaluation_alg, gamma=self.gamma, n_episodes=self.n_episodes, epsilon=self.epsilon)\n",
    "        mrp.fit(S, transition_matrix, R)\n",
    "        # return the policy value from the evaluation\n",
    "        return mrp.value_func()\n",
    "        \n",
    "    \n",
    "    def _build_transition_matrix_from_policy(self, S, P, policy):\n",
    "        transition_matrix = np.zeros((S, S))\n",
    "        for state, action in zip(range(S), policy):\n",
    "            transition_matrix[state] = P[action][state]\n",
    "            \n",
    "        return transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MarkovDecisionProcess(alg=POLICY_ITER, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mdp.policy_evaluation(S, P_non_deterministic, R, A, policy=PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.fit(S, P_non_deterministic, R, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdp.best_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp.value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp_iter = MarkovDecisionProcess(alg=VALUE_ITER, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
